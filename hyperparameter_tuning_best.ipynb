{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\user\\Downloads\\scgpt5.0\\scGPT\\tutorials\\..\\scgpt\\model\\model.py:24: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "c:\\Users\\user\\Downloads\\scgpt5.0\\scGPT\\tutorials\\..\\scgpt\\model\\multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "#from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from scgpt import prepare_data, prepare_dataloader, define_wandb_metrcis, evaluate, eval_testdata, train\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.model import MultiOmicTransformerModel\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.tokenizer import random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    task = 'multiomic',\n",
    "    seed=42,\n",
    "    dataset_name=\"BMMC\", # Dataset name\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../save/scGPT_human\", # Path to pre-trained model\n",
    "    freeze = False, #freeze\n",
    "    GEP=True, # Gene expression modelling\n",
    "    GEPC=True, # Gene expression modelling for cell objective\n",
    "    CLS=False,\n",
    "    ESC=False,\n",
    "    DAR = True, # DAR objective weight for batch correction\n",
    "    DSBN = False,  # Domain-spec batchnorm,\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    explicit_zero_prob = False,  # whether explicit bernoulli for zeros\n",
    "    ecs_thres=0,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0,\n",
    "    use_batch_labels = True,\n",
    "    use_mod = True,\n",
    "    per_seq_batch_sample = False,\n",
    "    epochs=25, # Default number of epochs for fine-tuning\n",
    "    input_layer_key = \"X_binned\", # Default expression value binning in data pre-processing\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    n_hvg = 1200,  # Default number of highly variable genes\n",
    "    n_hvp = 4000,\n",
    "    max_seq_len = 4001, # # Default n_hvg+1\n",
    "    lr=1e-3, # Default learning rate for fine-tuning\n",
    "    batch_size=16, # Default batch size for fine-tuning\n",
    "    layer_size=512,\n",
    "    nlayers=4,\n",
    "    nhead=8, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.95,  # Default rate for learning rate decay\n",
    "    save_eval_interval=5, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=False, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # Default setting: Automatic Mixed Precision\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_value = -1,\n",
    "    pad_value = -2,\n",
    "    include_zero_gene = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me1475736\u001b[0m (\u001b[33me1475736-nus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Downloads\\scgpt5.0\\scGPT\\tutorials\\wandb\\run-20241229_132017-izj52yow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e1475736-nus/scGPT/runs/izj52yow' target=\"_blank\">stellar-vortex-38</a></strong> to <a href='https://wandb.ai/e1475736-nus/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e1475736-nus/scGPT' target=\"_blank\">https://wandb.ai/e1475736-nus/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e1475736-nus/scGPT/runs/izj52yow' target=\"_blank\">https://wandb.ai/e1475736-nus/scGPT/runs/izj52yow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'multiomic', 'seed': 42, 'dataset_name': 'BMMC', 'do_train': True, 'load_model': '../save/scGPT_human', 'freeze': False, 'GEP': True, 'GEPC': True, 'CLS': False, 'ESC': False, 'DAR': True, 'DSBN': False, 'mask_ratio': 0.4, 'explicit_zero_prob': False, 'ecs_thres': 0, 'dab_weight': 1.0, 'use_batch_labels': True, 'use_mod': True, 'per_seq_batch_sample': False, 'epochs': 25, 'input_layer_key': 'X_binned', 'n_bins': 51, 'n_hvg': 1200, 'n_hvp': 4000, 'max_seq_len': 4001, 'lr': 0.001, 'batch_size': 16, 'layer_size': 512, 'nlayers': 4, 'nhead': 8, 'dropout': 0.2, 'schedule_ratio': 0.95, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'pad_token': '<pad>', 'mask_value': -1, 'pad_value': -2, 'include_zero_gene': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"thread\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save\\dev_BMMC-Dec29-13-20\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [config[\"pad_token\"], \"<cls>\", \"<eoc>\"]\n",
    "\n",
    "dataset_name = config[\"dataset_name\"]\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'BMMC':\n",
    "    adata = sc.read(r'C:\\Users\\user\\Downloads\\scgpt5.0\\scGPT\\tutorials\\BMMC_processed.h5ad')\n",
    "    # subset to first 3 donors with B, Mono and T cell subtypes\n",
    "    adata = adata[adata.obs.DonorID.isin([10886, 11466, 12710]) & adata.obs.cell_type.isin(np.unique(adata.obs.cell_type.values)[:17])]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str).astype('category')\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_batch = le.fit_transform(adata.obs['batch'].values)\n",
    "    adata.obs[\"batch_id\"] =  encoded_batch\n",
    "    adata.obs[\"str_batch\"] = adata.obs[\"batch_id\"].astype('category')\n",
    "    adata_protein = adata[:, adata.var.feature_types.isin(['ADT'])].copy()\n",
    "    adata_protein.var.index = ['p_' + i for i in adata_protein.var.index]\n",
    "    adata = adata[:, adata.var.feature_types.isin(['GEX'])].copy()\n",
    "    data_is_raw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"use_mod\"]:\n",
    "    gene_rna_df = pd.DataFrame(index = adata.var.index.tolist())\n",
    "    gene_rna_df['mod'] = 'RNA'\n",
    "    gene_protein_df = pd.DataFrame(index = adata_protein.var.index.tolist())\n",
    "    gene_protein_df['mod'] = 'Protein'\n",
    "    gene_loc_df = pd.concat([gene_rna_df, gene_protein_df])\n",
    "    gene_loc_df['mod'] = gene_loc_df['mod'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 12587/13953 genes in vocabulary of size 60697.\n"
     ]
    }
   ],
   "source": [
    "if config[\"load_model\"] is not None:\n",
    "    model_dir = Path(config[\"load_model\"])\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    old_vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - WARNING - No batch_key is provided, will use all cells for HVG selection.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=1,  # step 1\n",
    "    filter_cell_by_counts=1,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config[\"n_hvg\"],  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config[\"n_bins\"],  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor_protein = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=0,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=False,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=False,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=None,\n",
    "    binning=config[\"n_bins\"],  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor_protein(adata_protein, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = np.concatenate([adata.layers[\"X_binned\"], adata_protein.layers[\"X_binned\"]], axis=1)\n",
    "adata = AnnData(\n",
    "    X=data_combined,\n",
    "    obs=adata.obs,\n",
    "    var=pd.DataFrame(index=adata.var_names.tolist() + adata_protein.var_names.tolist()),\n",
    "    layers={\"X_binned\": data_combined,}\n",
    ")\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 12578 × 1334\n",
       "    obs: 'GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors', 'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts', 'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'is_train', 'celltype', 'batch_id', 'str_batch', 'n_counts'\n",
       "    var: 'gene_name'\n",
       "    layers: 'X_binned'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"per_seq_batch_sample\"]:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = (\n",
    "    adata.layers[config[\"input_layer_key\"]].A\n",
    "    if issparse(adata.layers[config[\"input_layer_key\"]])\n",
    "    else adata.layers[config[\"input_layer_key\"]]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"use_mod\"]:\n",
    "    mod_type = np.array([gene_loc_df.loc[g, 'mod'] for g in genes])\n",
    "    vocab_mod = Vocab(VocabPybind(np.unique(gene_loc_df['mod']).tolist() + special_tokens, None))\n",
    "    vocab_mod.set_default_index(vocab_mod[\"<pad>\"])\n",
    "    mod_type = np.array(vocab_mod(list(mod_type)), dtype=int)\n",
    "    ntokens_mod = len(vocab_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num of non_zero genes: 687\n",
      "min num of non_zero genes: 206\n",
      "average num of non_zero genes: 341.5295053003534\n",
      "99% quantile num of non_zero genes: 510.0\n",
      "max original values: 50\n",
      "average original non_zero values: 25.3482856428962\n",
      "99% quantile original non_zero values: 49.0\n",
      "num of celltypes: 17\n"
     ]
    }
   ],
   "source": [
    "num_of_non_zero_genes = [\n",
    "    np.count_nonzero(train_data[i]) for i in range(train_data.shape[0])\n",
    "]\n",
    "print(f\"max num of non_zero genes: {np.max(num_of_non_zero_genes)}\")\n",
    "print(f\"min num of non_zero genes: {np.min(num_of_non_zero_genes)}\")\n",
    "print(f\"average num of non_zero genes: {np.mean(num_of_non_zero_genes)}\")\n",
    "print(\n",
    "    f\"99% quantile num of non_zero genes: {np.quantile(num_of_non_zero_genes, 0.99)}\"\n",
    ")\n",
    "print(f\"max original values: {np.max(train_data)}\")\n",
    "print(\n",
    "    f\"average original non_zero values: {np.mean(train_data[np.nonzero(train_data)])}\"\n",
    ")\n",
    "print(\n",
    "    f\"99% quantile original non_zero values: {np.quantile(train_data[np.nonzero(train_data)], 0.99)}\"\n",
    ")\n",
    "print(f\"num of celltypes: {num_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"load_model\"] is None:\n",
    "    vocab = Vocab(VocabPybind(genes + special_tokens, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "else:\n",
    "    pretrained_genes = [g for g in genes + special_tokens if g in old_vocab]\n",
    "    new_genes = [g for g in genes + special_tokens if g not in old_vocab]\n",
    "    gene_ids_pretrained = np.array(old_vocab(pretrained_genes), dtype=int)\n",
    "    # https://discuss.pytorch.org/t/expand-an-existing-embedding-and-linear-layer-nan-loss-value/55670/2\n",
    "    # Retrieve pretrained weights\n",
    "    vocab = Vocab(VocabPybind(pretrained_genes + new_genes, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set number of samples: 11320, \n",
      "\t feature length: 688\n",
      "valid set number of samples: 1258, \n",
      "\t feature length: 663\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=config[\"max_seq_len\"],\n",
    "    vocab=vocab,\n",
    "    pad_token=config[\"pad_token\"],\n",
    "    pad_value=config[\"pad_value\"],\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=config[\"include_zero_gene\"],\n",
    "    mod_type=mod_type if config[\"use_mod\"] else None,\n",
    "    vocab_mod=vocab_mod if config[\"use_mod\"] else None,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=config[\"max_seq_len\"],\n",
    "    vocab=vocab,\n",
    "    pad_token=config[\"pad_token\"],\n",
    "    pad_value=config[\"pad_value\"],\n",
    "    append_cls=True,\n",
    "    include_zero_gene=config[\"include_zero_gene\"],\n",
    "    mod_type=mod_type if config[\"use_mod\"] else None,\n",
    "    vocab_mod=vocab_mod if config[\"use_mod\"] else None,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dict = torch.load(model_file, map_location=device)\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "\n",
    "embsize = config[\"layer_size\"]\n",
    "nhead = config[\"nhead\"]\n",
    "nlayers = config[\"nlayers\"]\n",
    "d_hid = config[\"layer_size\"]\n",
    "\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def load_pretrained(nlayers):\n",
    "    model = MultiOmicTransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid, \n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config[\"dropout\"],\n",
    "        pad_token=config[\"pad_token\"],\n",
    "        pad_value=config[\"pad_value\"],\n",
    "        do_mvc=config[\"GEPC\"],\n",
    "        do_dab=config[\"DAR\"],\n",
    "        use_batch_labels=config[\"use_batch_labels\"],\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=config[\"DSBN\"],\n",
    "        n_input_bins=config[\"n_bins\"],\n",
    "        ecs_threshold=config[\"ecs_thres\"],\n",
    "        explicit_zero_prob=config[\"explicit_zero_prob\"],\n",
    "        use_fast_transformer=config[\"fast_transformer\"],\n",
    "        pre_norm=config[\"pre_norm\"],\n",
    "        use_mod=config[\"use_mod\"],\n",
    "        ntokens_mod=ntokens_mod if config[\"use_mod\"] else None,\n",
    "        vocab_mod=vocab_mod if config[\"use_mod\"] else None,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        pretrained_emb_weights = model_dict['encoder.embedding.weight'][gene_ids_pretrained, :]\n",
    "        model.encoder.embedding.weight.data[:len(pretrained_genes), :] = pretrained_emb_weights\n",
    "        model.encoder.enc_norm.weight.data = model_dict['encoder.enc_norm.weight']\n",
    "    # ntokens = len(vocab)\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'epochs': 40,\n",
       " 'learning_rate': 0.0003212508440696567,\n",
       " 'nlayers': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "nlayers_ss = hp.choice(\"n_layers\", np.arange(2, 8))\n",
    "learning_rate_ss = hp.uniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"nlayers\": hp.choice(\"n_layers\", list(np.arange(2, 8))),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "    \"epochs\": hp.choice(\"epochs\", list(np.arange(20, 50, 10))),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", list(np.arange(16, 48, 16))),\n",
    "}\n",
    "\n",
    "sample(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sample):\n",
    "    nhead = config.nhead\n",
    "    embsize = config.layer_size\n",
    "    d_hid = config.layer_size\n",
    "\n",
    "    nlayers = sample[\"nlayers\"]\n",
    "    model = load_pretrained(nlayers)\n",
    "\n",
    "    if config[\"GEP\"] and config[\"GEPC\"]:\n",
    "        criterion_gep_gepc = masked_mse_loss\n",
    "    if config[\"CLS\"]:\n",
    "        criterion_cls = nn.CrossEntropyLoss()\n",
    "    if config[\"DAR\"]:\n",
    "        criterion_dab = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=sample[\"learning_rate\"], \n",
    "        eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=config[\"schedule_ratio\"])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config[\"amp\"])\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(\n",
    "            tokenized_train=tokenized_train, \n",
    "            tokenized_valid=tokenized_valid, \n",
    "            train_batch_labels=train_batch_labels,\n",
    "            valid_batch_labels=valid_batch_labels,\n",
    "            config=config,\n",
    "            epoch=epoch,\n",
    "            sort_seq_batch=config.per_seq_batch_sample)\n",
    "        \n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "            per_seq_batch_sample=config.per_seq_batch_sample\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "            per_seq_batch_sample=config.per_seq_batch_sample\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model=model,\n",
    "                loader=train_loader,\n",
    "                vocab=vocab,\n",
    "                criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "                criterion_dab=criterion_dab if config.DAR else None,\n",
    "                criterion_cls=criterion_cls if config.CLS else None,\n",
    "                scaler=scaler,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                config=config,\n",
    "                logger=logger,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "\n",
    "        train_loss = evaluate(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            vocab=vocab,\n",
    "            criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "            criterion_dab=criterion_dab if config.DAR else None,\n",
    "            criterion_cls=criterion_cls if config.CLS else None,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss = evaluate(\n",
    "            model=model,\n",
    "            loader=valid_loader,\n",
    "            vocab=vocab,\n",
    "            criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "            criterion_dab=criterion_dab if config.DAR else None,\n",
    "            criterion_cls=criterion_cls if config.CLS else None,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.4f} | \"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "            logger.info(f\"Saving model to {save_dir}\")\n",
    "            torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "            # eval on testdata\n",
    "            results = eval_testdata(\n",
    "                model = best_model,\n",
    "                adata_t = adata_sorted if config.per_seq_batch_sample else adata,\n",
    "                gene_ids = gene_ids,\n",
    "                vocab = vocab,\n",
    "                config = config,\n",
    "                logger = logger,\n",
    "                include_types=[\"cls\"],\n",
    "            )\n",
    "            results[\"batch_umap\"].savefig(\n",
    "                save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "            )\n",
    "\n",
    "            results[\"celltype_umap\"].savefig(\n",
    "                save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "            )\n",
    "            \n",
    "            metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "            metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "                str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "                caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "            )\n",
    "\n",
    "            metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "                str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "                caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "            )\n",
    "            metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "            wandb.log(metrics_to_log)\n",
    "            wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    return val_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, space_eval\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from functools import partial\n",
    "\n",
    "optimised = fmin(\n",
    "    fn = train_model,\n",
    "    space = search_space,\n",
    "    algo = partial(\n",
    "        tpe.suggest,\n",
    "        n_startup_jobs = 10,\n",
    "    ),\n",
    "    max_evals = 20,\n",
    "    rstate = np.random.default_rng(42),\n",
    ")\n",
    "\n",
    "#use one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1, 'epochs': 0, 'learning_rate': 0.0004614941433845602, 'n_layers': 5}\n"
     ]
    }
   ],
   "source": [
    "print(optimised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size = 32 and epoch = 20\n",
    "above shows list index\n",
    "\n",
    "best_config = space_eval(search_space, optimised)\n",
    "print(f\"Best configuration: {best_config}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
